# Agentic AI RAG System - Features Documentation

## Table of Contents
1. [System Overview](#system-overview)
2. [Architecture](#architecture)
3. [Core Features](#core-features)
4. [Database System](#database-system)
5. [Memory Management](#memory-management)
6. [API Key Management](#api-key-management)
7. [Document Management](#document-management)
8. [LLM Integration](#llm-integration)
9. [User Interface](#user-interface)
10. [Security Features](#security-features)

---

## System Overview

The Agentic AI RAG System is a Retrieval-Augmented Generation (RAG) application that allows users to upload documents and interact with them through natural language conversations. The system retrieves relevant information from uploaded documents and uses Large Language Models (LLMs) to generate accurate, context-aware responses.

### Key Capabilities
- Upload and index multiple document formats (TXT, PDF, DOCX, MD)
- Natural language querying with context-aware responses
- Multi-provider LLM support (Groq, OpenAI, Google Gemini, Deepseek)
- Intelligent conversation memory with automatic summarization
- Secure API key storage with encryption
- Modern web interface with real-time chat

---

## Architecture

### System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User Interface Layer                      â”‚
â”‚                      (Streamlit Web App)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Chat         â”‚  â”‚ Document     â”‚  â”‚ API Settings â”‚        â”‚
â”‚  â”‚ Interface    â”‚  â”‚ Management   â”‚  â”‚ Modal        â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Application Layer                           â”‚
â”‚                        (src/app.py)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Query Orchestration & Document Operations           â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                    â”‚                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   RAG System    â”‚  â”‚  Memory Manager â”‚  â”‚  LLM Query  â”‚
    â”‚  (rag_system)   â”‚  â”‚ (memory_manager)â”‚  â”‚ (llm_query) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                    â”‚               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚               â”‚
    â”‚  ChromaDB       â”‚          â”‚               â”‚
    â”‚  (Vector Store) â”‚          â”‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚               â”‚
                                 â”‚               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              External Services                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
    â”‚  â”‚   Groq   â”‚  â”‚  OpenAI  â”‚  â”‚  Gemini  â”‚  ...      â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Diagram

```
User Query â†’ Chat Interface â†’ Application Layer
                                      â†“
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚                       â”‚
                    RAG System              Memory Manager
                          â”‚                       â”‚
                    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”          Token Counting
                    â”‚           â”‚          & History
              ChromaDB    Embedding            â†“
              Search      Model          Prepare Context
                    â”‚           â”‚               â”‚
                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚
                          â†“                     â†“
                 Retrieved Context    â† â”€ â”€ â”€ â”€ â”˜
                          â”‚
                          â†“
                    LLM Query Module
                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
                    â”‚           â”‚
            Selected Provider   â”‚
            (Groq/OpenAI/etc)  â”‚
                    â”‚           â”‚
                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                          â†“
                    LLM Response
                          â”‚
                          â†“
                    Chat Interface
                          â”‚
                          â†“
                    Display to User
```

---

## Core Features

### 1. Retrieval-Augmented Generation (RAG)

The RAG system combines vector similarity search with LLM generation to provide accurate answers based on uploaded documents.

**How it works:**

1. **Document Processing**: When documents are uploaded, they are:
   - Split into chunks using LlamaIndex's `SimpleDirectoryReader`
   - Converted to vector embeddings using HuggingFace model `BAAI/bge-small-en-v1.5`
   - Stored in ChromaDB with metadata

2. **Query Processing**: When a user asks a question:
   - The query is converted to a vector embedding
   - ChromaDB performs cosine similarity search to find relevant document chunks
   - Top N results (default: 3) are retrieved
   - Results are formatted and sent to the LLM along with the query

3. **Response Generation**: The LLM generates a response based on:
   - Retrieved document context
   - User's question
   - Conversation history (managed by Memory Manager)
   - System prompt with strict retrieval guidelines

**Quality Control:**
- **Distance threshold: 0.7** (ChromaDB uses cosine distance where lower = more similar)
- Documents with distance > 0.7 are rejected as not relevant enough
- If documents exceed the threshold, returns: "I don't have enough information to answer this question."
- System prompt enforces strict adherence to provided context only

---

## Database System

### ChromaDB Vector Database

The system uses ChromaDB as the vector database for storing and retrieving document embeddings.

#### Database Configuration

```
Database Location: ./chroma_db/
Collection Name: document_embeddings
Distance Metric: Cosine Distance (lower = more similar)
Embedding Model: BAAI/bge-small-en-v1.5
Embedding Dimension: 384
```

#### Database Structure

```
ChromaDB Collection
â”œâ”€â”€ IDs: Unique identifiers for each document chunk
â”‚   Format: "{filename}_{hash(text)}"
â”‚
â”œâ”€â”€ Documents: Raw text content of each chunk
â”‚
â”œâ”€â”€ Embeddings: 384-dimensional vectors
â”‚   Generated by: HuggingFace BGE-small-en-v1.5
â”‚
â””â”€â”€ Metadata: Associated information
    â”œâ”€â”€ source: Original filename
    â”œâ”€â”€ file_type: MIME type
    â”œâ”€â”€ file_size: Size in bytes
    â””â”€â”€ upload_type: "user_upload"
```

#### Database Operations

**1. Document Insertion**
- Each document is chunked automatically
- Multiple chunks per document are stored with unique IDs
- Duplicate detection prevents re-uploading same document

**2. Vector Search**
```
Query Text â†’ Embedding Model â†’ Query Vector (384-dim)
                                      â†“
                        ChromaDB Cosine Distance Search
                                      â†“
                        Top N Results with Distance Scores
                                      â†“
                        Filter by Distance Threshold (> 0.7 rejected)
                        (Lower distance = more similar)
                                      â†“
                        Format Results for LLM
```

**3. Document Management**
- Retrieve all documents with metadata
- Delete documents by ID
- Clear entire database
- Group chunks by original filename

#### Persistence

ChromaDB is configured with persistent storage:
- Database files stored in `./chroma_db/`
- Data persists between application restarts
- SQLite backend (`chroma.sqlite3`) for metadata
- HNSW index for fast similarity search

---

## Memory Management

The Memory Manager is one of the most sophisticated features, handling conversation history with intelligent token management and automatic summarization.

### Memory Management Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Memory Manager                           â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Conversation History (chat_history)         â”‚  â”‚
â”‚  â”‚  [ChatMessage(role, content), ...]                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚        Token Counting (tiktoken)              â”‚        â”‚
â”‚  â”‚  Encoding: cl100k_base (GPT-4 tokenizer)      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                          â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚      Threshold Monitoring                     â”‚        â”‚
â”‚  â”‚  â€¢ Token Limit: 8000                          â”‚        â”‚
â”‚  â”‚  â€¢ Summarize Threshold: 70% (5600 tokens)     â”‚        â”‚
â”‚  â”‚  â€¢ Question Threshold: 20% (1600 tokens)      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                          â”‚                                  â”‚
â”‚         Decision: Summarization Needed?                    â”‚
â”‚                          â”‚                                  â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚         â”‚                                 â”‚               â”‚
â”‚    YES (>70%)                        NO (<70%)            â”‚
â”‚         â”‚                                 â”‚               â”‚
â”‚         â†“                                 â†“               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Summarize   â”‚                   â”‚ Use Full     â”‚      â”‚
â”‚  â”‚ Older       â”‚                   â”‚ History      â”‚      â”‚
â”‚  â”‚ Messages    â”‚                   â”‚              â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Token Management Parameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| **TOKEN_LIMIT** | 8000 | Maximum total tokens per LLM request |
| **SUMMARIZE_THRESHOLD** | 0.7 (70%) | Trigger summarization at 5600 tokens |
| **QUESTION_THRESHOLD** | 0.2 (20%) | Max question size: 1600 tokens |

### How Memory Management Works

#### 1. Token Counting

The system uses `tiktoken` library with the `cl100k_base` encoding (GPT-4 tokenizer) for accurate token counting:

```
Text â†’ tiktoken.encode() â†’ Token Count
```

Every message in the conversation history is counted:
- System prompts
- User questions
- Assistant responses
- Retrieved context
- Summarization text

#### 2. Memory States

**State 1: Normal Operation (< 70% of limit)**
- Full conversation history is included in the prompt
- All previous exchanges are available for context
- No summarization needed

**State 2: Approaching Limit (â‰¥ 70% of limit)**
- Automatic summarization is triggered
- Older messages (all except last 6) are summarized
- Recent exchanges (last 3 user-assistant pairs) remain in full
- Summary replaces older messages in the context

**State 3: Question Too Large (> 20% of limit)**
- User's question exceeds 1600 tokens
- System returns error: "Your question is too long. Please reduce your input to continue the conversation."
- Question is rejected before processing

**State 4: Hard Limit Reached**
- Total tokens exceed 8000 even after summarization
- System returns: "The conversation has become too long. Please start a new conversation to continue."
- User must clear chat to continue

#### 3. Summarization Process

When summarization is triggered:

```
Step 1: Identify Messages to Summarize
        â””â”€> All messages except last 6 (last 3 exchanges)

Step 2: Format for Summarization
        â”œâ”€> User: [message 1]
        â”œâ”€> Assistant: [message 2]
        â””â”€> ... (conversation flow)

Step 3: Send to LLM with Summarization Prompt
        â””â”€> Uses selected provider (Groq/OpenAI/etc)
        â””â”€> Temperature: 0.1 (focused, deterministic)

Step 4: Generate Summary
        â””â”€> 50-200 words
        â””â”€> Captures main topics and key Q&A
        â””â”€> Maintains conversational flow

Step 5: Replace Older Messages
        â””â”€> Keep last 6 messages (3 exchanges)
        â””â”€> Insert summary as system message
        â””â”€> Continue conversation with reduced history
```

#### Summarization Prompt

The system uses a specialized prompt for summarization:

**Requirements:**
- Capture main topics discussed
- Include key questions and answers
- Maintain conversational flow and context
- Keep concise (50-200 words)
- Focus on information useful for future context
- Use clear, professional language

#### 4. Context Preparation Pipeline

```
prepare_context() Function
        â”‚
        â”œâ”€> Count tokens for all components:
        â”‚   â”œâ”€> Query tokens
        â”‚   â”œâ”€> System prompt tokens
        â”‚   â”œâ”€> Context tokens (from RAG)
        â”‚   â””â”€> Chat history tokens
        â”‚
        â”œâ”€> Check question threshold (20%)
        â”‚   â””â”€> If exceeded: Return error
        â”‚
        â”œâ”€> Check summarization threshold (70%)
        â”‚   â”œâ”€> If exceeded AND history > 6 messages:
        â”‚   â”‚   â”œâ”€> Summarize older messages
        â”‚   â”‚   â”œâ”€> Keep last 6 messages
        â”‚   â”‚   â””â”€> Set needs_summarization flag
        â”‚   â””â”€> Else: Use full history
        â”‚
        â”œâ”€> Assemble final message list:
        â”‚   â”œâ”€> System prompt
        â”‚   â”œâ”€> Summary (if created)
        â”‚   â”œâ”€> Recent chat history
        â”‚   â””â”€> Current query + context
        â”‚
        â”œâ”€> Final validation (total < 8000)
        â”‚   â””â”€> If exceeded: Return hard limit error
        â”‚
        â””â”€> Return: (messages, error, needs_summarization)
```

#### 5. Memory Persistence

**Within Session:**
- All messages stored in `memory_manager.chat_history` list
- Persists throughout application session
- Accessible to all query operations

**Between Sessions:**
- Memory is NOT persisted to disk
- New session starts with empty history
- User can manually clear with "Clear Chat" button

---

## API Key Management

### Secure API Key Storage

The system implements a multi-layered security approach for handling API keys.

#### Security Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              User Input (API Settings Modal)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Session State (st.session_state)           â”‚
â”‚         Temporary storage during application run        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    Save Request
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               LocalStorage (local_storage.py)           â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Encryption Process                             â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ 1. Get API key from session state        â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ 2. Apply Fernet encryption               â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ 3. Store encrypted value                 â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Persistent Storage (JSON file)                  â”‚
â”‚  Location: ~/.streamlit_cache/agentic_ai/api_keys.json â”‚
â”‚  Content: Encrypted API keys                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Encryption Implementation

**Encryption Method: Fernet (Symmetric Encryption)**

**Key Derivation:**
```
Username (from OS) â†’ PBKDF2HMAC
                        â”œâ”€> Algorithm: SHA256
                        â”œâ”€> Iterations: 100,000
                        â”œâ”€> Salt: Username bytes
                        â””â”€> Output: 32-byte key

32-byte key â†’ Base64 encode â†’ Fernet cipher
```

**Encryption Process:**
```
Plain API Key â†’ Fernet.encrypt() â†’ Encrypted bytes â†’ Base64 â†’ Store as string
```

**Decryption Process:**
```
Stored string â†’ Base64 decode â†’ Encrypted bytes â†’ Fernet.decrypt() â†’ Plain API Key
```

#### Storage Location

```
Windows: C:\Users\{username}\.streamlit_cache\agentic_ai\api_keys.json
Linux/Mac: ~/.streamlit_cache/agentic_ai/api_keys.json
```

#### File Structure

```json
{
  "api_keys": {
    "groq": "encrypted_key_string_here",
    "openai": "encrypted_key_string_here",
    "gemini": "encrypted_key_string_here",
    "deepseek": "encrypted_key_string_here"
  },
  "selected_provider": "groq"
}
```

#### API Key Lifecycle

**1. Initial Setup**
- User opens API Settings modal
- Enters API key for desired provider
- Key stored in session state (temporary)
- User clicks "Save & Close"
- Key is encrypted and saved to disk

**2. Application Startup**
- LocalStorage reads encrypted keys from file
- Keys are decrypted and loaded into session state
- Selected provider is restored
- Keys remain in memory during session

**3. Usage During Queries**
- APIKeyManager retrieves key from session state
- Key passed directly to LLM provider API
- No environment variable fallback
- No logging or transmission of raw keys

**4. Updates**
- User can update keys anytime via API Settings
- Old encrypted value is overwritten
- New encryption applied to updated key

**5. Clearing**
- User can clear individual keys with "X" button
- Empty string is encrypted and stored
- Effectively removes the key

### Supported LLM Providers

| Provider | Model | API Endpoint | Get API Key |
|----------|-------|--------------|-------------|
| **Groq** | llama-3.1-8b-instant | Groq API | https://console.groq.com |
| **OpenAI** | gpt-3.5-turbo | OpenAI API | https://platform.openai.com/api-keys |
| **Google Gemini** | gemini-pro | Gemini API | https://ai.google.dev |
| **Deepseek** | deepseek-chat | Deepseek API | https://platform.deepseek.com |

### Provider Selection

The system uses a single active provider at a time:
- User selects provider via radio buttons in API Settings
- Selection saved to persistent storage
- All queries use the selected provider
- Can switch providers anytime without losing keys

---

## Document Management

### Supported File Formats

| Format | Extension | Description | Layout Support |
|--------|-----------|-------------|----------------|
| **Text** | .txt | Plain text files | N/A |
| **PDF** | .pdf | Adobe PDF documents | **Enhanced** - Preserves layout, tables, columns |
| **Word** | .docx | Microsoft Word documents | Basic |
| **Markdown** | .md | Markdown formatted text | N/A |
| **HTML** | .html | HTML web pages | Basic |

**PDF Processing**: The system uses PyMuPDFReader for advanced PDF processing, which:
- Preserves document layout and formatting
- Extracts tables with structure intact
- Handles multi-column layouts correctly
- Maintains text positioning and flow

### Document Upload Process

```
User Selects File(s) â†’ Streamlit File Uploader
                               â”‚
                               â†“
                    Click "Upload Documents"
                               â”‚
                               â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                     â”‚
            Check for Duplicates    Create Temp File
                    â”‚                     â”‚
                    â”‚                     â†“
            Name exists?         LlamaIndex Reader
                    â”‚                     â”‚
            YES â”€â†’ Cancel          Extract Text
                    â”‚                     â”‚
            NO â”€â”€â†’ Continue         Split into Chunks
                                         â”‚
                                         â†“
                              Generate Embeddings
                              (HuggingFace BGE)
                                         â”‚
                                         â†“
                              Store in ChromaDB
                                         â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                    â”‚
            Assign Unique IDs                    Save Metadata
            (filename_hash)                      (source, type, size)
                    â”‚                                    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â†“
                              Display Success Message
                                         â”‚
                                         â†“
                              Refresh Document List
```

### Duplicate Prevention

The system prevents duplicate document uploads:

1. **Check by Filename**: Before processing, system checks if a document with the same name already exists
2. **Cancel if Exists**: If duplicate found, returns message: "Document '{name}' already exists. Upload cancelled."
3. **No Hash Check**: Only filename is checked (not content hash), so renamed duplicates would be allowed

### Document Chunking

Documents are automatically chunked by LlamaIndex's `SimpleDirectoryReader`:
- Automatic chunk size determination
- Maintains semantic coherence
- Each chunk stored as separate database entry
- All chunks from same document share source metadata

### Document Metadata

Each document chunk stores the following metadata:

```python
{
    "source": "filename.pdf",           # Original filename
    "file_type": "application/pdf",     # MIME type
    "file_size": 45678,                 # Size in bytes
    "upload_type": "user_upload"        # Distinguishes user uploads from system docs
}
```

### Document Display

The sidebar displays documents grouped by filename:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Uploaded Documents:             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ document1.pdf            ğŸ—‘ï¸    â”‚
â”‚ document2.txt            ğŸ—‘ï¸    â”‚
â”‚ report.docx              ğŸ—‘ï¸    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Display Information:**
- Document name
- Delete button (ğŸ—‘ï¸) for each document
- All chunks deleted together when document is removed

### Bulk Operations

**Clear Database:**
- Single button to remove all documents
- Confirmation via spinner
- Deletes all document chunks
- Resets database to empty state

---

## LLM Integration

### Multi-Provider Architecture

The system supports four LLM providers with a unified interface.

#### LLM Query Flow

```
User Query + Retrieved Context
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Memory Manager                â”‚
â”‚  â€¢ Count tokens                   â”‚
â”‚  â€¢ Check thresholds               â”‚
â”‚  â€¢ Prepare context                â”‚
â”‚  â€¢ Handle summarization           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
    Format Messages for LLM
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ System Prompt       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Summary (if any)    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Chat History        â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Context + Query     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
    Select Provider
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚          â”‚
    â†“          â†“          â†“          â†“
  Groq     OpenAI    Gemini    Deepseek
    â”‚          â”‚          â”‚          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
              LLM Response
                     â”‚
                     â†“
         Store in Memory Manager
                     â”‚
                     â†“
         Return to User Interface
```

### Provider-Specific Implementation

#### 1. Groq (Default Provider)

**Configuration:**
```
Model: llama-3.1-8b-instant
Library: llama-index-llms-groq
Temperature: 0.0 (deterministic)
```

**Implementation:**
- Uses LlamaIndex's Groq integration
- Native ChatMessage format support
- Fastest inference time
- Recommended for real-time interactions

#### 2. OpenAI

**Configuration:**
```
Model: gpt-3.5-turbo
Library: openai (official Python SDK)
Temperature: 0.0 (deterministic)
```

**Implementation:**
- Converts ChatMessage to OpenAI format
- Format: {"role": "system|user|assistant", "content": "..."}
- Most widely tested and reliable
- Good balance of speed and quality

#### 3. Google Gemini

**Configuration:**
```
Model: gemini-pro
Library: google-generativeai
Temperature: N/A (using default)
```

**Implementation:**
- Converts messages to prompt string
- Format: "System: ...\n\nUser: ...\n\nAssistant: ..."
- Single prompt generation (not native chat format)
- Good for multilingual support

#### 4. Deepseek

**Configuration:**
```
Model: deepseek-chat
Library: openai (via custom base_url)
Base URL: https://api.deepseek.com
Temperature: 0.0 (deterministic)
```

**Implementation:**
- OpenAI-compatible API
- Uses OpenAI library with custom endpoint
- Same message format as OpenAI
- Cost-effective alternative

### System Prompt Configuration

The system uses a sophisticated system prompt that enforces strict retrieval-only behavior:

**Key Directives:**
1. **Context-Only Responses**: Only use information from provided context
2. **Language Detection**: Detect question language and respond in same language
3. **Fallback Response**: If answer not in context, return: "I don't have enough information to answer this question."
4. **No Hallucination**: Never use world knowledge, assumptions, or inferences
5. **Professional Style**: Clear, structured, formal tone
6. **Plain Text Format**: UTF-8 text without Markdown formatting

**Defense Layers:**
- No speculation allowed
- No combining partial information
- No answering meta-questions about system behavior
- Strict context boundaries

### Temperature Setting

All providers use **temperature: 0.0** (or closest equivalent):
- Ensures deterministic, consistent responses
- Reduces randomness and creativity
- Prioritizes accuracy over diversity
- Best for factual question-answering

### Error Handling

Each provider has comprehensive error handling:

```
Try:
    â”œâ”€> Validate API key exists
    â”œâ”€> Prepare messages
    â”œâ”€> Call provider API
    â””â”€> Return response

Catch:
    â”œâ”€> API key missing: "Error: {PROVIDER} API key not configured..."
    â”œâ”€> API error: "Error: Failed to get response from {PROVIDER}: {error}"
    â””â”€> Unknown provider: "Error: Unknown API provider '{provider}'"
```

---

## User Interface

### Streamlit Web Application

The UI is built with Streamlit and custom CSS for a modern, responsive experience.

### UI Components Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   streamlit_app.py (Main)                   â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚                   â”‚ Initialize States   â”‚                   â”‚
â”‚                   â”‚ Apply Styles        â”‚                   â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                             â”‚                                â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚         â”‚                   â”‚                   â”‚           â”‚
â”‚         â†“                   â†“                   â†“           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Sidebar   â”‚    â”‚   Header    â”‚    â”‚    Chat     â”‚     â”‚
â”‚  â”‚ (Document  â”‚    â”‚  (Title +   â”‚    â”‚  Interface  â”‚     â”‚
â”‚  â”‚Management) â”‚    â”‚  Buttons)   â”‚    â”‚             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                   â”‚                   â”‚           â”‚
â”‚         â†“                   â†“                   â†“           â”‚
â”‚  - File Upload      - Clear Chat        - Message Display  â”‚
â”‚  - Document List    - Download PDF      - Chat Input       â”‚
â”‚  - API Settings     - Title             - Copy Buttons     â”‚
â”‚  - Clear DB                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layout Structure

**Page Configuration:**
```
Layout: Centered
Page Title: RAG-based AI Assistant
Icon: ğŸ¤–
```

**Main Areas:**

1. **Sidebar** (Left, 15% min-width)
   - Document management controls
   - File uploader
   - Document list with delete buttons
   - API Settings button
   - Clear database button

2. **Main Area** (Center)
   - Chat header with title and action buttons
   - Chat message display area
   - Chat input at bottom

3. **Modal Overlay** (When opened)
   - API Settings configuration
   - Centered modal design
   - Provider selection tabs
   - API key inputs

### Chat Interface Features

#### Message Display

**User Messages (Right-aligned):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                             ğŸ‘¤     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ User's question appears here...      â”‚         â”‚
â”‚  â”‚                                      â”‚         â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚         â”‚
â”‚  â”‚ 14:30                          ğŸ“‹   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Assistant Messages (Left-aligned):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ğŸ¤–                                             â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚         â”‚ Assistant's response with markdown   â”‚  â”‚
â”‚         â”‚ support for **bold**, *italic*, etc. â”‚  â”‚
â”‚         â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”‚
â”‚         â”‚ 14:31                          ğŸ“‹   â”‚  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Visual Design Features

**Color Scheme:**
- User messages: Red accent (`rgba(239, 68, 68, 0.5)`)
- Assistant messages: Orange accent (`rgba(251, 146, 60, 0.5)`)
- Dark theme background with transparency
- Subtle gradients and shadows

**Animations:**
- Slide-in fade animation for new messages
- Hover effects on message bubbles
- Scale animations on icons
- Smooth transitions (0.3s cubic-bezier)

**Typography:**
- Message content: Times New Roman (formal style)
- Font size: 1.05rem with 1.85 line height
- Code blocks: Courier New/Consolas monospace
- Markdown support: Bold, italic, inline code

**Interactive Elements:**
- Copy button (ğŸ“‹) on each message
- Visual feedback on copy (âœ“ appears)
- Hover effects increase opacity
- Timestamp display in footer

#### Chat History Export

**PDF Download Feature:**
- "ğŸ“„ Download" button in header
- Generates PDF with full conversation
- Includes timestamp of generation
- Formatted with roles (User/Assistant)
- Handles special characters safely

### API Settings Modal

**Modal Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âš™ï¸ API Keys Configuration            âœ•    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Groq  â”‚ â”‚ OpenAI â”‚ â”‚ Gemini â”‚ â”‚Deep â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Enter your {Provider} API Key              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    X   â”‚
â”‚  â”‚ â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢ â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                             â”‚
â”‚  Current Active Provider: GROQ              â”‚
â”‚                                             â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚          â”‚ Save & Close â”‚                  â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Modal Features:**
- Tab-based provider selection
- Active provider highlighted with checkmark
- Password-masked inputs
- Individual clear buttons (X) for each key
- Save confirmation message
- Click outside does NOT close (explicit close button)

### Responsive Behaviors

**Loading States:**
- "Thinking..." spinner during query processing
- Disabled input while model responds
- Visual feedback prevents double submissions

**Auto-scroll:**
- Automatically scrolls to latest message
- Smooth scrolling behavior
- Refocus on input after response

**Input Management:**
- Chat input disabled during processing
- Shows "Model is responding..." when thinking
- Auto-focus on input field when ready
- Enter to send, Shift+Enter for new line

---

## Security Features

### 1. API Key Protection

**Storage Security:**
- Fernet symmetric encryption (AES-128)
- PBKDF2 key derivation (100,000 iterations)
- User-specific salt (OS username)
- Keys never stored in plain text

**Usage Security:**
- No environment variable fallback
- No logging of raw API keys
- No transmission except to provider APIs
- Keys remain in memory only during session

**UI Security:**
- Password-masked inputs (type="password")
- No show/hide toggle for security
- Text selection disabled on password fields
- Individual key clearing mechanism

### 2. Input Validation

**Query Validation:**
- Maximum question length: 20% of token limit (1600 tokens)
- Character encoding validation
- HTML escaping for display
- XSS prevention

**File Upload Validation:**
- Extension whitelist: .txt, .pdf, .docx, .md
- Duplicate filename detection
- Temporary file handling with cleanup
- Error handling for corrupt files

### 3. Data Isolation

**Local Storage:**
- User-specific cache directory
- No cloud storage of sensitive data
- Encrypted persistence only
- Clear separation of user data

**Database Isolation:**
- Local ChromaDB instance
- No external database connections
- User-specific collections possible
- Manual clear functionality

### 4. Error Message Safety

**Information Disclosure Prevention:**
- Generic error messages to users
- Detailed errors logged (not displayed)
- No API key leakage in errors
- No system path exposure

### 5. Session Management

**State Isolation:**
- Streamlit session state per user
- No cross-session data leakage
- Automatic session cleanup
- Memory cleared on session end

---

## Advanced Features

### Conversation Continuity

The system maintains conversation continuity through:

1. **Context Preservation**: Chat history informs future responses
2. **Reference Resolution**: Can understand pronouns and previous topics
3. **Progressive Disclosure**: Build on previous answers
4. **Topic Tracking**: Maintains thread of conversation

### Multi-Document Querying

Users can ask questions spanning multiple documents:

1. **Cross-Document Search**: Retrieves from all uploaded documents
2. **Source Attribution**: Each response shows which documents were used
3. **Relevance Ranking**: Top N most relevant chunks across all documents
4. **Context Merging**: Combines information from multiple sources

### Language Support

The system is designed to be multilingual:

1. **Language Detection**: System prompt instructs LLM to detect query language
2. **Response Language**: Responds in the same language as the question
3. **Embedding Model**: BGE supports multiple languages
4. **Unicode Support**: Full UTF-8 support throughout

### Real-Time Feedback

Users receive immediate feedback:

1. **Typing Indicator**: Input disabled while processing
2. **Loading Spinners**: Visual indication of background work
3. **Success Messages**: Confirmation of actions (upload, delete, clear)
4. **Error Messages**: Clear explanations when things go wrong
5. **Message Timestamps**: Shows when each message was sent

---

## System Limitations

### Token Limits

- **Maximum conversation**: 8000 tokens total
- **Maximum question**: 1600 tokens (20% of limit)
- **Summarization trigger**: 5600 tokens (70% of limit)
- **Hard reset**: Required when hitting absolute limit

### Document Limits

- **File size**: Limited by available memory
- **Chunk size**: Determined by LlamaIndex (not configurable in current version)
- **Embedding time**: Proportional to document size
- **Search results**: Fixed at top 3 chunks per query

### Performance Considerations

- **Initial load**: First query slower due to model loading
- **Concurrent users**: Single ChromaDB instance (local only)
- **Memory usage**: Grows with conversation length
- **Storage space**: ChromaDB grows with document count

### Provider Limitations

- **Rate limits**: Subject to each provider's rate limits
- **API costs**: Varies by provider (user responsible)
- **Model availability**: Dependent on provider service status
- **Model capabilities**: Each model has different strengths

---

## Future Enhancement Opportunities

Based on the architecture, potential improvements include:

1. **Persistent Conversations**: Save chat history between sessions
2. **User Authentication**: Multi-user support with separate data
3. **Cloud Database**: MongoDB or cloud ChromaDB for scalability
4. **Advanced Analytics**: Document usage statistics, query patterns
5. **Collaborative Features**: Share documents and conversations
6. **Custom Embedding Models**: Allow user to select embedding model
7. **Configurable Chunking**: User-defined chunk size and overlap
8. **Streaming Responses**: Real-time token streaming from LLMs
9. **Multi-Modal Support**: Images, tables, charts in documents
10. **Export Options**: More formats (Word, JSON, etc.)

---

## Conclusion

The Agentic AI RAG System is a comprehensive document intelligence platform that combines modern vector search, sophisticated memory management, and flexible LLM integration to provide accurate, context-aware responses to user queries. The system's architecture prioritizes security, user experience, and reliability while maintaining extensibility for future enhancements.

Key strengths:
- **Robust memory management** with automatic summarization
- **Secure API key handling** with encryption
- **Multi-provider LLM support** for flexibility
- **Modern, responsive UI** with attention to detail
- **Comprehensive error handling** and validation
- **Local-first architecture** for data privacy

The system is production-ready for personal use or small teams, with a clear path for scaling to enterprise requirements through the addition of user authentication, cloud storage, and collaborative features.
